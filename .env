# =========================
# OCI GenAI (Oracle Cloud)
# =========================
# OCI user OCID used for signing requests to Oracle Cloud.
OCI_USER_OCID=ocid1.user.oc1..aaaaaaaa5cq3iewffep5nzqb7qzoe6mpj45gt4kndvzwvuxzzavpbiucqqaq
# Path to your OCI API private key used with the fingerprint below.
OCI_KEY_FILE=/Users/shadab/.oci/oci_api_key.pem
# Fingerprint of the public key you uploaded to OCI console for the above private key.
OCI_KEY_FINGERPRINT=de:50:15:13:af:bd:76:fa:f4:77:ad:d4:af:70:a5:d6
# Tenancy OCID of your Oracle Cloud account.
OCI_TENANCY_OCID=ocid1.tenancy.oc1..aaaaaaaafhegmvy2da7xzh2b5jbmhdkfr4cr4e37m5filt4zgxs6mfl7icua
# Region where your OCI resources live (e.g., ap-sydney-1, us-chicago-1).
OCI_REGION=us-chicago-1
# Compartment OCID where GenAI resources are located.
OCI_COMPARTMENT_OCID=ocid1.compartment.oc1..aaaaaaaacoqxp2n77ra2343maw2px4rlrtzqaw5ord6be2cbrbwlrpqwegxa
# Specific Generative AI model OCID to use for Oracle GenAI endpoints.
OCI_GENAI_MODEL_OCID=ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceyayjawvuonfkw2ua4bob4rlnnlhs522pafbglivtwlfzta
# If using ~/.oci/config profiles, set the profile name (default: DEFAULT)
# OCI_CONFIG_PROFILE=DEFAULT

# =========================
# PostgreSQL / PGVector
# =========================
# Hostname/IP of your PostgreSQL server.
#COGNEO_DB_HOST=10.150.2.230
# PostgreSQL port (default 5432).
#COGNEO_DB_PORT=5432
# Database user for ingestion and query.
#COGNEO_DB_USER=postgres
# Database user password (keep secret; do not commit .env).
#COGNEO_DB_PASSWORD=RAbbithole1234##
# Database name/schema to use.
#COGNEO_DB_NAME=postgres

# Optional full SQLAlchemy DSN (overrides the above when set). Example:
#COGNEO_DB_URL='postgresql+psycopg2://postgres:postgres@localhost:5432/postgres'

# =========================
# Oracle 23ai (optional)
# =========================
# Uncomment and set if connecting to Oracle 23ai.
ORACLE_DB_USER=admin
ORACLE_DB_PASSWORD=RAbbithole1234##
ORACLE_DB_DSN='(description= (retry_count=20)(retry_delay=3)(address=(protocol=tcps)(port=1521)(host=localhost))(connect_data=(service_name=g9b8049aad9c64c_aisearchdemo_high.adb.oraclecloud.com))(security=(ssl_server_dn_match=no)))'
# ORACLE_WALLET_LOCATION=/path/to/wallet/dir

# =========================
# Backend Switch (Default: Postgres)
# =========================
# Set database backend: postgres | oracle
#COGNEO_DB_BACKEND=postgres
# To use Oracle backend, uncomment the line below and provide Oracle vars above:
COGNEO_DB_BACKEND=oracle
# Optionally provide a single DSN for Oracle SQLAlchemy:
#ORACLE_SQLALCHEMY_URL=oracle+oracledb://user:password@myadb_high
# Oracle AI Vector Search (26ai) — optional index/bootstrap
COGNEO_ORA_APPROX=1
COGNEO_ORA_AUTO_VECTOR_INDEX=0
COGNEO_ORA_INDEX_TYPE=HNSW            # HNSW | IVF
COGNEO_ORA_DISTANCE=COSINE            # COSINE | EUCLIDEAN | EUCLIDEAN_SQUARED | DOT | MANHATTAN | HAMMING
COGNEO_ORA_ACCURACY=90                # Target accuracy for approximate search
COGNEO_ORA_INDEX_PARALLEL=1           # Parallel threads for index build
# HNSW parameters
COGNEO_ORA_HNSW_NEIGHBORS=16
COGNEO_ORA_HNSW_EFCONSTRUCTION=200
# IVF parameters
#COGNEO_ORA_IVF_PARTITIONS=100

# =========================
# Embeddings / Models
# =========================
# Sentence-Transformers or HF model name for embeddings.
COGNEO_EMBED_MODEL=nomic-ai/nomic-embed-text-v1.5
## If using Offline Model then use below for COGNEO_EMBED_MODEL
#COGNEO_EMBED_MODEL="/opt/hf-cache/nomic-embed-text-v1.5"
# Embedding dimensionality for pgvector column (must match model, e.g., 768).
COGNEO_EMBED_DIM=768
# Optional HF cache directory on fast disk for model weights.
# HF_HOME=/fast/ssd/hf_cache
# (Optional) Disable tokenizer multithreading warnings (correct var name).
# TOKENIZERS_PARALLELISM=false
# Allow custom HF model code (required for some repos like nomic-ai). 1=allow
COGNEO_TRUST_REMOTE_CODE=1
# Optional: pin a specific HF revision/tag/commit to avoid pulling latest code/assets (stabilizes cold starts)
# Example: "refs/tags/v1.5" or a specific commit hash supported by the repo
# COGNEO_EMBED_REV=refs/tags/v1.5
# Optional: run strictly from local HF cache (no network). Set to 1 after you prefetch the model.
# COGNEO_HF_LOCAL_ONLY=1
# If you fully prefetch and want to enforce offline mode across transformers, you can also set:
# TRANSFORMERS_OFFLINE=1
# Maximum sequence length for HF fallback (tokenizer truncation), default 512
# COGNEO_EMBED_MAXLEN=512
# Internal flags for embedder behavior (space-separated, e.g. "trust_remote_code")
# COGNEO_EMBEDDER_FLAGS=

# (Optional) HF transfer engine toggle; disable to avoid rust transfer, enable for resumable high-speed
# HF_HUB_ENABLE_HF_TRANSFER=0

# FULL OFFLINE MODE FOR EMBEDDINGS (Git LFS) — HOW TO
# 1) Prepare a persistent cache folder:
#    sudo mkdir -p /opt/hf-cache
#    sudo chown -R $USER:$USER /opt/hf-cache
#    export HF_HOME=/opt/hf-cache
# 2) Install Git LFS and clone the model:
#    sudo apt-get update && sudo apt-get install -y git git-lfs
#    git lfs install
#    cd /opt/hf-cache
#    git clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
#    cd nomic-embed-text-v1.5
#    git lfs pull
#    (This fetches model.safetensors and related files; you can optionally remove onnx/ to save space.)
# 3) In .env, set the model path and offline flags when you want fully offline runs:
#    COGNEO_EMBED_MODEL="/opt/hf-cache/nomic-embed-text-v1.5"
#    TRANSFORMERS_OFFLINE=1
#    COGNEO_HF_LOCAL_ONLY=1
#    HF_HOME=/opt/hf-cache
# 4) Load the .env into your shell before running:
#    set -a; source .env; set +a
# 5) Validate:
#    python3 -c "from embedding.embedder import Embedder; e=Embedder(); print('model=',e.model_name,'dim=',e.dimension)"

# =========================
# Ingestion / Worker Flags
# =========================
# Auto-run create_all_tables() on import (set 0 when schema is pre-created).
COGNEO_AUTO_DDL=1
# Per-GPU embedding batch size (worker adapts down on OOM automatically).
COGNEO_EMBED_BATCH=32
# Light schema init for first runs on new instances to avoid heavy DB backfills and large index builds.
# 0 = full schema init (backfill FTS, build vector index); 1 = skip heavy steps (you can run them later).
COGNEO_SCHEMA_LIGHT_INIT=1
# Unbuffered Python output so logs flush immediately (recommended for long-running workers/orchestrator).
PYTHONUNBUFFERED=1
# CPU parallelism for parse+chunk stage (Stage A, multiprocessing):
# - Set to number of CPU worker processes per GPU worker.
# - 0 or unset -> auto: min(physical_cores-1, 8), at least 1
# - Set >1 to enable the pipelined mode (CPU pool feeding GPU).
COGNEO_CPU_WORKERS=8
# Prefetch buffer size (number of files prepared ahead of GPU embedding).
# - Controls how many CPU-prepared files are queued to keep the GPU fed.
# - 0 or unset -> default 64
# - Increase on larger RAM/GPUs (e.g., 128–256) to maximize overlap.
COGNEO_PIPELINE_PREFETCH=128
# Sort each worker's assigned files by size (descending) to reduce tail latency.
# 1 = enabled (default), 0 = disable (keep natural order).
COGNEO_SORT_WORKER_FILES=1
# Limit how many URLs are expanded into literal SQL when using --show-sql
# in tools/delete_url_records.py. Remaining URLs (if any) are omitted from output.
# Default: 20
COGNEO_SHOWSQL_MAXURLS=20

# Include per-file metrics & timings (parse_ms, chunk_ms, embed_ms, insert_ms).
COGNEO_LOG_METRICS=1
# Emit detailed error records to {child}.errors.ndjson (1=enabled).
COGNEO_ERROR_DETAILS=1
# Include Python tracebacks in error records (0=omit, 1=include).
COGNEO_ERROR_TRACE=1
# Print periodic DB document/embedding counts (debug only; default 0)
# COGNEO_DEBUG_COUNTS=0

# =========================
# Chunking / Timeouts
# =========================
# Per-stage deadlines (seconds). Stuck files are aborted and will fallback or error.
COGNEO_TIMEOUT_PARSE=30
COGNEO_TIMEOUT_CHUNK=60
COGNEO_TIMEOUT_EMBED_BATCH=180
# Per-file DB insert deadline (seconds). Retries occur with exponential backoff.
COGNEO_TIMEOUT_INSERT=120
# Regex timeout for semantic_chunker operations (ms); requires the 'regex' package.
# Lower this if you see stalls during "chunk start". Defaults to 200ms if unset.
COGNEO_REGEX_TIMEOUT_MS=200
# Enable RCTS fallback for unstructured generic text (requires langchain-text-splitters + tiktoken).
COGNEO_USE_RCTS_GENERIC=1
# Character-window fallback chunker sizes, used when semantic chunking times out/errors.
COGNEO_FALLBACK_CHARS_PER_CHUNK=4000
COGNEO_FALLBACK_OVERLAP_CHARS=200
# Enable character-window fallback when chunking times out/errors (default: 1)
# COGNEO_FALLBACK_CHUNK_ON_TIMEOUT=1

# =========================
# Database Pooling / Timeouts (Prod)
# =========================
# Size of SQLAlchemy connection pool per process (workers open their own pools).
COGNEO_DB_POOL_SIZE=10
# Extra connections allowed beyond pool_size during bursts.
COGNEO_DB_MAX_OVERFLOW=20
# Recycle connections periodically to avoid server idle timeouts (seconds).
COGNEO_DB_POOL_RECYCLE=1800
# Max seconds to wait for a free pooled connection before raising.
COGNEO_DB_POOL_TIMEOUT=30
# Server-side statement timeout (milliseconds) applied to new connections (optional).
# COGNEO_DB_STATEMENT_TIMEOUT_MS=60000
# Client-side per-file DB SELECT deadline (seconds) for session-file row checks (prevents pre-parse stalls).
COGNEO_TIMEOUT_SELECT=30
# Max retries for transient DB errors on per-file SELECT/INSERT with exponential backoff.
COGNEO_DB_MAX_RETRIES=5

# =========================
# GPU / Performance Tuning (recommended)
# =========================
# Mitigate CUDA memory fragmentation and allocator churn (helps avoid stalls/OOM thrash).
# See: https://pytorch.org/docs/stable/notes/cuda.html#environment-variables
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
# Limit math library threads per worker to reduce CPU contention on multi-GPU runs.
OMP_NUM_THREADS=1
MKL_NUM_THREADS=1
OPENBLAS_NUM_THREADS=1
NUMEXPR_NUM_THREADS=1
# Reduce glibc heap arena fragmentation under heavy multi-threaded loads.
MALLOC_ARENA_MAX=2
# Pin workers to specific GPU(s) when launching (set per-process)
# CUDA_VISIBLE_DEVICES=0

# =========================
# API / Backend (optional, if using FastAPI/Gradio/Streamlit auth)
# =========================
# Basic auth user/pass for local FastAPI admin endpoints or internal tooling.
# FASTAPI_API_USER=legal_api
# FASTAPI_API_PASS=letmein
# Public base URL for the API/UI if needed by clients.
# COGNEO_API_URL=http://localhost:8000

# =========================
# Notes
# =========================
# - Do NOT commit real secrets to source control.
# - Use environment managers (systemd, Docker secrets, cloud secret stores) in production.
# - Ensure COGNEO_EMBED_DIM matches the selected embedding model.
# - For fully offline embeddings: set COGNEO_EMBED_MODEL to the local path (e.g., /opt/hf-cache/nomic-embed-text-v1.5)
#   and set TRANSFORMERS_OFFLINE=1 and COGNEO_HF_LOCAL_ONLY=1 after prefetching via Git LFS as above.

# =========================
# Vector Backend Selection
# =========================
# Choose which vector/search backend to use for API endpoints:
# - postgres     : PostgreSQL + pgvector (default)
# - oracle       : Oracle Database 26ai VECTOR
# - opensearch   : OpenSearch KNN (HNSW cosine) via opensearch-py adapter
# If both COGNEO_VECTOR_BACKEND and COGNEO_DB_BACKEND are set, COGNEO_VECTOR_BACKEND wins.
# COGNEO_VECTOR_BACKEND=postgres

# =========================
# OpenSearch Backend (optional)
# =========================
# Only used when COGNEO_VECTOR_BACKEND=opensearch.
# Host endpoint (single-node or load-balanced):
OPENSEARCH_HOST=http://localhost:9200
# (Optional) Basic auth credentials:
# OPENSEARCH_USER=admin
# OPENSEARCH_PASS=admin
# Index name for chunks + vectors:
OPENSEARCH_INDEX=cogneo_chunks
# Embed dim for knn_vector mapping (must match COGNEO_EMBED_DIM):
# COGNEO_EMBED_DIM=768

# =========================
# AWS Bedrock (LLM) — optional
# =========================
# Uses AWS default credential/provider chain (.aws/credentials, env, IAM role).
# Region for Bedrock runtime (e.g., us-east-1, us-west-2):
AWS_REGION=us-east-1
# Optional alternate region var used by some SDKs:
# AWS_DEFAULT_REGION=us-east-1
# Default model ID (example: Anthropic Claude 3 Haiku):
BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0

# =========================
# Security & Auth
# =========================
# Auth mode for API routes that require auth:
# - basic : HTTP Basic (FASTAPI_API_USER/PASS)
# - jwt   : Bearer JWT tokens issued by /auth/token
# COGNEO_AUTH_MODE=jwt

# JWT configuration (used when COGNEO_AUTH_MODE=jwt). Install PyJWT.
# COGNEO_JWT_SECRET=changeme
# COGNEO_JWT_ALG=HS256
# COGNEO_JWT_EXPIRE_MIN=60

# Basic auth credentials (also used as JWT gate in /auth/token):
# FASTAPI_API_USER=legal_api
# FASTAPI_API_PASS=letmein

# =========================
# CORS / Rate limiting / Prometheus
# =========================
# CORS toggle + policy:
COGNEO_CORS_ENABLE=1
COGNEO_CORS_ALLOW_ORIGINS=*
COGNEO_CORS_ALLOW_METHODS=*
COGNEO_CORS_ALLOW_HEADERS=*
COGNEO_CORS_ALLOW_CREDENTIALS=1

# Simple in-process rate limiting (per-client IP):
COGNEO_RATE_LIMIT_ENABLE=0
COGNEO_RATE_LIMIT_REQUESTS=60
COGNEO_RATE_LIMIT_WINDOW_S=60

# Prometheus metrics via /metrics (starlette-exporter):
COGNEO_PROMETHEUS_ENABLE=1
